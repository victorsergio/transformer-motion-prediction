{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"13CQicc7RO4Aiwtic1sIbyFst1PrXajnX","authorship_tag":"ABX9TyOc2Ooa8auH13aghJnd7Xak"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"mCL7Jxa9QGpI","executionInfo":{"status":"ok","timestamp":1697029379191,"user_tz":-120,"elapsed":486530,"user":{"displayName":"victor sergio peÃ±aloza","userId":"17418028661329428856"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"38435f99-1a2e-4bc8-d8bf-88c0d45f5374"},"outputs":[{"output_type":"stream","name":"stdout","text":["05 file preprocessed ...\n","16 file preprocessed ...\n","28 file preprocessed ...\n","/content/drive/Othercomputers/My Laptop/github-repositories/transformer-multi/data/inD-dataset-v1.0/data/validation.joblib split was created.\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from shapely.geometry import Point\n","from shapely.geometry.polygon import Polygon\n","import shapely\n","import math\n","from sklearn.preprocessing import LabelEncoder\n","from joblib import load\n","from joblib import dump\n","from sklearn.preprocessing import StandardScaler\n","\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","\n","class Occupancy:\n","    # Wrapper to be able to store an occupancy grid (numpy array nxm)\n","    # inside a pandas dataframe row\n","\n","    def __init__(self, grid):\n","        self.grid = grid\n","\n","    def shape(self):\n","        return self.grid.shape\n","\n","\n","def get_index(x_values,y_values,grid_x_values, grid_y_values):\n","\n","    # - Returns an occupancy grid filled with the corresponding values\n","    #   from the dataset\n","\n","    m = grid_y_values.shape[0]\n","    n = grid_x_values.shape[0]\n","\n","    # create empty grid\n","    grid = np.zeros((m+1,n+1), dtype=int)\n","\n","    # Get index for a list of points (coordinates)\n","    xindex = np.digitize(x_values, grid_x_values)\n","    yindex = np.digitize(y_values, grid_y_values)\n","\n","    # fill the grid\n","    for (col, row) in zip(xindex, yindex):\n","        grid[row,col] = 1\n","\n","    # Remove borders of the grid, remove outside elements, we are only interested\n","    # in the center of the intersection\n","    grid = grid[1:-1, 1:-1]\n","    return grid\n","\n","\n","def create_grid(location, resolution=5):\n","\n","    # - Initialize an empty occupancy grid\n","    # :param scene: scene for the ocuppancy grid\n","    # :param resolution: dimension of each grid's cell (meters)\n","    # :return x: list of x coordinates ofeach vertex of the grid\n","    # :return y: list of y coordinates ofeach vertex of the grid\n","\n","    location4_vertices = [(126.9483,-84.46665),\n","                          (170.6603,-60.82265),\n","                          (161.4343,-34.85665),\n","                          (112.8543,-54.99365)]\n","\n","    location1_vertices = [(55.6866,-45.34663),\n","                          (66.2296,-34.218634),\n","                          (58.5466,-19.87363),\n","                          (43.6466,-31.58663)]\n","\n","    location2_vertices = [(41.0835,-38.32758),\n","                          (65.7185,-36.00858),\n","                          (56.3135,-16.72358),\n","                          (30.7125,-21.01958)]\n","\n","    location3_vertices = [(57.5857,-46.3137),\n","                          (65.8517,-33.4267),\n","                          (44.7197,-18.6437),\n","                          (36.0587,-29.6787)]\n","\n","\n","    if (location == 'location1'):\n","        center_vertices = location1_vertices\n","    if (location == 'location2'):\n","        center_vertices = location2_vertices\n","    if (location == 'location3'):\n","        center_vertices = location3_vertices\n","    if (location == 'location4'):\n","        center_vertices = location4_vertices\n","\n","\n","    CENTER = Polygon(center_vertices)\n","\n","    xmin, ymin, xmax, ymax = CENTER.bounds\n","\n","    # construct the rectangle of points\n","    x, y = np.round(np.meshgrid(np.arange(xmin, xmax, resolution), np.arange(ymin, ymax, resolution)),4)\n","\n","    return x, y\n","\n","\n","def grid_labelling(df, location='location1', resolution=5):\n","\n","# Calculates the occupancy grid for each frame\n","# param location: recording location\n","# param resolution: size of the grid cell in meters\n","# return: a labelled dataframe with occupancy grids for each frame\n","\n","    # Add a new column to df to store the occupancy as object\n","    df = df.assign(occupancy=None)\n","    df['occupancy'] = df.occupancy.astype(object)\n","\n","    # Create an empty grid placeholder\n","    x_grid, y_grid = create_grid(location,resolution=resolution)\n","\n","    groups = df.groupby(['frame'])\n","\n","    for name, group in groups:\n","        g = get_index(group.xCenter.values,group.yCenter.values,x_grid[0],np.transpose(y_grid)[0])\n","\n","        # Create an Occupancy object and Store it in each frame row\n","        occupancy = Occupancy(g)\n","        df.loc[df['frame'] == name, 'occupancy'] = occupancy\n","\n","    return df\n","\n","\n","def split_sequences(df, max_len = 20):\n","\n","    # Split sequences to feed the neural network in training\n","\n","    # Add a new column to df to store the new indexes\n","    df[\"sequenceId\"] = np.nan\n","\n","    current_id = 0\n","\n","    groups = df.groupby(['trackId'])\n","\n","    for name, group in groups:\n","        # how many subsequences of max length can be formed\n","        n_subsequences = math.ceil(len(group.index)/max_len)\n","\n","        # create the indexes for the splits\n","        new_ids = np.arange(current_id, current_id + n_subsequences, 1)\n","        # create the splits (windowing)\n","        new_ids = np.repeat(new_ids, max_len)\n","\n","        # match new ids length with index original length\n","        new_ids = new_ids[0:len(group.index)]\n","\n","        assert len(new_ids) == len(group.index)\n","\n","        # update the dataframe with the new window indexes\n","        serie = pd.Series(new_ids, index=group.index, name='sequenceId')\n","        df.update(serie)\n","\n","        current_id = current_id + n_subsequences\n","\n","    df['sequenceId'] = df.sequenceId.astype('int64')\n","\n","    return df\n","\n","def downsample(df, step=10):\n","\n","    # Downsample the dataset, take a sample every 10 samples\n","    # original sample, 25 fps -> 80 frames = 3.2 seconds\n","    # downsample to 8 frames = 3.2 seconds\n","\n","    df = df.groupby(['trackId'], as_index=False).apply(lambda group: group.iloc[::step]) .reset_index(drop = True, inplace = False)\n","    return df\n","\n","def create_synthetic_id(df):\n","    df['globalSequenceId'] = df['recordingId'].astype('str')+\"-\"+df['sequenceId'].astype('str')\n","\n","    le = LabelEncoder()\n","    df['globalSequenceId'] = le.fit_transform(df['globalSequenceId'])\n","\n","    return df\n","\n","def filter_columns(df):\n","    return df[['globalSequenceId', 'xCenter', 'yCenter', 'xCenterRelative', 'yCenterRelative', 'heading']]\n","\n","def standarize_data(df, reload_scaler = False):\n","\n","    scaler = StandardScaler()\n","\n","    if(reload_scaler == True):\n","        scaler = load(data_path+'scaler.joblib')\n","\n","\n","    df_meta = df[['globalSequenceId','xCenter','yCenter']]\n","    df_values = df[[\"xCenterRelative\", \"yCenterRelative\", \"heading\"]]\n","\n","    scaled_values = scaler.fit_transform(df_values)\n","    df_values = pd.DataFrame(scaled_values, columns = df_values.columns)\n","\n","    # ignore_index=False to keep column header\n","    df = pd.concat([df_meta, df_values], axis=1, ignore_index=False)\n","\n","    if(reload_scaler == False):\n","        dump(scaler, data_path+'scaler.joblib')\n","\n","    return df\n","\n","def preprocess_file(data_path, track_number):\n","\n","    # Read dataset\n","    tracks_csv = track_number+\"_tracks.csv\"\n","    meta_csv = track_number+\"_tracksMeta.csv\"\n","\n","    df = pd.read_csv(data_path+tracks_csv, sep=',')\n","    df_meta = pd.read_csv(data_path+meta_csv, sep=',')\n","\n","    # Concat meta + tracks\n","    df_meta = df_meta['class']\n","    df = df.join(df_meta, on='trackId')\n","\n","    # Calculate the occupancy grids\n","    df = grid_labelling(df,location=recToLocation[track_number],resolution=resolution)\n","\n","    # Convert to relative positions\n","    df['xCenterRelative'] = df.groupby(['trackId'])['xCenter'].diff().fillna(0)\n","    df['yCenterRelative'] = df.groupby(['trackId'])['yCenter'].diff().fillna(0)\n","\n","    # downsample\n","    df = downsample(df, step=10)\n","\n","    # Create subsequences\n","    df = split_sequences(df, max_len = max_len)\n","\n","    # Remove sequences shorter than max_len = 20\n","    df = df.groupby('sequenceId').filter(lambda x: len(x) >= max_len)\n","\n","    return df\n","\n","\n","def create_split(files, split_name, reload_scaler):\n","\n","    data = pd.DataFrame()\n","\n","    for f in files:\n","        df = preprocess_file(data_path, f)\n","        data = pd.concat([data, df], ignore_index=True)\n","        print(f\"{f} file preprocessed ...\")\n","\n","    # Create a unique id for the .csv\n","    data = create_synthetic_id(data)\n","    # Select only features of interest\n","    data = filter_columns(data)\n","    # Standarize data columns for training\n","    data = standarize_data(data, reload_scaler = reload_scaler)\n","\n","    dump(data, data_path+f\"{split_name}.joblib\")\n","    print(data_path+f\"{split_name}.joblib split was created.\")\n","\n","\n","# Preprocess all the files of the InD Dataset,\n","# calculate occupancy grids and create data splits\n","\n","data_path = \"/content/drive/Othercomputers/My Laptop/github-repositories/transformer-multi/data/inD-dataset-v1.0/data/\"\n","\n","max_len = 20\n","resolution = 5\n","\n","# Define recoding -> scene\n","recToLocation = dict.fromkeys(['00', '01', '02', '03', '04', '05', '06'], 'location4')\n","recToLocation.update(dict.fromkeys(['07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17'], 'location1'))\n","recToLocation.update(dict.fromkeys(['18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29'], 'location2'))\n","recToLocation.update(dict.fromkeys(['30', '31', '32'], 'location3'))\n","\n","# Define data splits\n","train_files = ['00', '01', '02', '03', '04',\n","               '07', '08', '09', '10', '11', '12', '13', '14', '15',\n","               '18', '19', '20', '21', '22', '23', '24', '25', '26', '27',\n","               '30'\n","               ]\n","\n","\n","validation_files = ['05','16', '28', '31']\n","test_files = ['06','17', '29','32']\n","\n","# Create train split, fit scaler\n","create_split(train_files, split_name=\"train\", reload_scaler=False)\n","#Create validation split\n","create_split(validation_files, split_name=\"validation\", reload_scaler=True)\n","# Create test split\n","create_split(test_files, split_name=\"test\", reload_scaler=True)"]}]}