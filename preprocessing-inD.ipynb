{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1aHgqxctupklTpIGdD6OY7vPfftYbQ7nP","authorship_tag":"ABX9TyMFpskmT2aIecn4a8R15B3Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"oHkE48eAJUFI","executionInfo":{"status":"ok","timestamp":1696325476239,"user_tz":-120,"elapsed":410,"user":{"displayName":"victor sergio peÃ±aloza","userId":"17418028661329428856"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","\n","\n","def mask_step(x, step):\n","\t\"\"\"\n","\tCreate a mask to only contain the step-th element starting from the first element. Used to downsample\n","\t\"\"\"\n","\tmask = np.zeros_like(x)\n","\tmask[::step] = 1\n","\treturn mask.astype(bool)\n","\n","\n","def downsample(df, step):\n","\t\"\"\"\n","\tDownsample data by the given step. Example, SDD is recorded in 30 fps, with step=30, the fps of the resulting\n","\tdf will become 1 fps. With step=12 the result will be 2.5 fps. It will do so individually for each unique\n","\tpedestrian (metaId)\n","\t:param df: pandas DataFrame - necessary to have column 'metaId'\n","\t:param step: int - step size, similar to slicing-step param as in array[start:end:step]\n","\t:return: pd.df - downsampled\n","\t\"\"\"\n","\tmask = df.groupby(['metaId'])['metaId'].transform(mask_step, step=step)\n","\treturn df[mask]\n","\n","\n","def filter_short_trajectories(df, threshold):\n","\t\"\"\"\n","\tFilter trajectories that are shorter in timesteps than the threshold\n","\t:param df: pandas df with columns=['x', 'y', 'frame', 'trackId', 'sceneId', 'metaId']\n","\t:param threshold: int - number of timesteps as threshold, only trajectories over threshold are kept\n","\t:return: pd.df with trajectory length over threshold\n","\t\"\"\"\n","\tlen_per_id = df.groupby(by='metaId', as_index=False).count()  # sequence-length for each unique pedestrian\n","\tidx_over_thres = len_per_id[len_per_id['frame'] >= threshold]  # rows which are above threshold\n","\tidx_over_thres = idx_over_thres['metaId'].unique()  # only get metaIdx with sequence-length longer than threshold\n","\tdf = df[df['metaId'].isin(idx_over_thres)]  # filter df to only contain long trajectories\n","\treturn df\n","\n","\n","def groupby_sliding_window(x, window_size, stride):\n","\tx_len = len(x)\n","\tn_chunk = (x_len - window_size) // stride + 1\n","\tidx = []\n","\tmetaId = []\n","\tfor i in range(n_chunk):\n","\t\tidx += list(range(i * stride, i * stride + window_size))\n","\t\tmetaId += ['{}_{}'.format(x.metaId.unique()[0], i)] * window_size\n","\t# temp = x.iloc()[(i * stride):(i * stride + window_size)]\n","\t# temp['new_metaId'] = '{}_{}'.format(x.metaId.unique()[0], i)\n","\t# df = df.append(temp, ignore_index=True)\n","\tdf = x.iloc()[idx]\n","\tdf['newMetaId'] = metaId\n","\treturn df\n","\n","\n","def sliding_window(df, window_size, stride):\n","\t\"\"\"\n","\tAssumes downsampled df, chunks trajectories into chunks of length window_size. When stride < window_size then\n","\tchunked trajectories are overlapping\n","\t:param df: df\n","\t:param window_size: sequence-length of one trajectory, mostly obs_len + pred_len\n","\t:param stride: timesteps to move from one trajectory to the next one\n","\t:return: df with chunked trajectories\n","\t\"\"\"\n","\tgb = df.groupby(['metaId'], as_index=False)\n","\tdf = gb.apply(groupby_sliding_window, window_size=window_size, stride=stride)\n","\tdf['metaId'] = pd.factorize(df['newMetaId'], sort=False)[0]\n","\tdf = df.drop(columns='newMetaId')\n","\tdf = df.reset_index(drop=True)\n","\treturn df\n","\n","\n","def split_at_fragment_lambda(x, frag_idx, gb_frag):\n","\t\"\"\" Used only for split_fragmented() \"\"\"\n","\tmetaId = x.metaId.iloc()[0]\n","\tcounter = 0\n","\tif metaId in frag_idx:\n","\t\tsplit_idx = gb_frag.groups[metaId]\n","\t\tfor split_id in split_idx:\n","\t\t\tx.loc[split_id:, 'newMetaId'] = '{}_{}'.format(metaId, counter)\n","\t\t\tcounter += 1\n","\treturn x\n","\n","\n","def split_fragmented(df):\n","\t\"\"\"\n","\tSplit trajectories when fragmented (defined as frame_{t+1} - frame_{t} > 1)\n","\tFormally, this is done by changing the metaId at the fragmented frame and below\n","\t:param df: DataFrame containing trajectories\n","\t:return: df: DataFrame containing trajectories without fragments\n","\t\"\"\"\n","\n","\tgb = df.groupby('metaId', as_index=False)\n","\t# calculate frame_{t+1} - frame_{t} and fill NaN which occurs for the first frame of each track\n","\tdf['frame_diff'] = gb['frame'].diff().fillna(value=1.0).to_numpy()\n","\tfragmented = df[df['frame_diff'] != 1.0]  # df containing all the first frames of fragmentation\n","\tgb_frag = fragmented.groupby('metaId')  # helper for gb.apply\n","\tfrag_idx = fragmented.metaId.unique()  # helper for gb.apply\n","\tdf['newMetaId'] = df['metaId']  # temporary new metaId\n","\n","\tdf = gb.apply(split_at_fragment_lambda, frag_idx, gb_frag)\n","\tdf['metaId'] = pd.factorize(df['newMetaId'], sort=False)[0]\n","\tdf = df.drop(columns='newMetaId')\n","\treturn df\n","\n","\n","def load_inD(path='inD-dataset-v1.0/data', scenes=[1], recordings=None):\n","\t'''\n","\tLoads data from inD Dataset. Makes the following preprocessing:\n","\t-filter out unnecessary columns\n","\t-filter out non-pedestrian\n","\t-makes new unique ID (column 'metaId') since original dataset resets id for each scene\n","\t-add scene name to column for visualization\n","\t-output has columns=['trackId', 'frame', 'x', 'y', 'sceneId', 'metaId']\n","\n","\t:param path: str - path to folder, default is 'data/inD'\n","\t:param scenes: list of integers - scenes to load\n","\t:param recordings: list of strings - alternative to scenes, load specified recordings instead, overwrites scenes\n","\t:return: DataFrame containing all trajectories from split\n","\t'''\n","\n","\tscene2rec = {1: ['00', '01', '02', '03', '04', '05', '06'],\n","\t\t\t\t 2: ['07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17'],\n","\t\t\t\t 3: ['18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29'],\n","\t\t\t\t 4: ['30', '31', '32']}\n","\n","\trec_to_load = []\n","\tfor scene in scenes:\n","\t\trec_to_load.extend(scene2rec[scene])\n","\tif recordings is not None:\n","\t\trec_to_load = recordings\n","\tdata = []\n","\tfor rec in rec_to_load:\n","\t\t# load csv\n","\t\ttrack = pd.read_csv(os.path.join(path, '{}_tracks.csv'.format(rec)))\n","#\t\ttrack = track.drop(columns=['trackLifetime', 'heading', 'width', 'length', 'xVelocity', 'yVelocity',\n","#\t\t\t\t\t\t\t\t\t'xAcceleration', 'yAcceleration', 'lonVelocity', 'latVelocity',\n","#\t\t\t\t\t\t\t\t\t'lonAcceleration', 'latAcceleration'])\n","\n","\t\ttrack = track.drop(columns=['trackLifetime', 'width', 'length',\n","\t\t\t\t\t\t\t\t\t'xAcceleration', 'yAcceleration', 'lonVelocity', 'latVelocity',\n","\t\t\t\t\t\t\t\t\t'lonAcceleration', 'latAcceleration'])\n","\n","\n","\t\ttrack_meta = pd.read_csv(os.path.join(path, '{}_tracksMeta.csv'.format(rec)))\n","\n","\t\t# Filter non-pedestrians\n","\t\t#pedestrians = track_meta[(track_meta['class'] != 'pedestrian') & (track_meta['class'] != 'bicycle') ]   #change for have sequences of all types of objects\n","\t\t#track = track[track['trackId'].isin(pedestrians['trackId'])]\n","\n","\t\ttrack['rec&trackId'] = [str(recId) + '_' + str(trackId).zfill(6) for recId, trackId in\n","\t\t\t\t\t\t\t\tzip(track.recordingId, track.trackId)]\n","\t\ttrack['sceneId'] = rec\n","\t\ttrack['yCenter'] = -track['yCenter']\n","\n","\t\t# Filter all trajectories outside the scene frame, ie negative values\n","\t\ttrack = track[(track['yCenter'] >= 0) & (track['xCenter'] >= 0)]\n","\n","\t\tdata.append(track)\n","\n","\tdata = pd.concat(data, ignore_index=True)\n","\n","\trec_trackId2metaId = {}\n","\tfor i, j in enumerate(data['rec&trackId'].unique()):\n","\t\trec_trackId2metaId[j] = i\n","\tdata['metaId'] = [rec_trackId2metaId[i] for i in data['rec&trackId']]\n","\tdata = data.drop(columns=['rec&trackId', 'recordingId'])\n","\tdata = data.rename(columns={'xCenter': 'x', 'yCenter': 'y'})\n","\n","\tcols_order = ['sceneId','metaId','trackId', 'frame', 'x', 'y', 'xVelocity','yVelocity','heading']\n","\tdata = data.reindex(columns=cols_order)\n","\n","\treturn data\n","\n","\n","def load_and_window_inD(step, window_size, stride, scenes, recordings, path='inD-dataset-v1.0/data'):\n","\t\"\"\"\n","\tHelper function to aggregate loading and preprocessing in one function. Preprocessing contains:\n","\t- Split fragmented trajectories\n","\t- Downsample fps\n","\t- Filter short trajectories below threshold=window_size\n","\t- Sliding window with window_size and stride\n","\t:param step (int): downsample factor, step=25 means 1fps on inD\n","\t:param window_size (int): Timesteps for one window\n","\t:param stride (int): How many timesteps to stride in windowing. If stride=window_size then there is no overlap\n","\t:param scenes (list of int): Which scenes to load, inD has 4 scenes\n","\t:return pd.df: DataFrame containing the preprocessed data\n","\t\"\"\"\n","\trec2scene = {'00': 'scene1', '01': 'scene1', '02': 'scene1', '03': 'scene1', '04': 'scene1', '05': 'scene1',\n","\t\t\t\t '06': 'scene1',\n","\t\t\t\t '07': 'scene2', '08': 'scene2', '09': 'scene2', '10': 'scene2', '11': 'scene2', '12': 'scene2',\n","\t\t\t\t '13': 'scene2', '14': 'scene2', '15': 'scene2', '16': 'scene2', '17': 'scene2',\n","\t\t\t\t '18': 'scene3', '19': 'scene3', '20': 'scene3', '21': 'scene3', '22': 'scene3', '23': 'scene3',\n","\t\t\t\t '24': 'scene3', '25': 'scene3', '26': 'scene3', '27': 'scene3', '28': 'scene3', '29': 'scene3',\n","\t\t\t\t '30': 'scene4', '31': 'scene4', '32': 'scene4'}\n","\n","\tdf = load_inD(path=path, recordings=recordings)\n","\t# inD is already perfectly continuous\n","\tdf = downsample(df, step=step)\n","\tdf = filter_short_trajectories(df, threshold=window_size)\n","\tdf = sliding_window(df, window_size=window_size, stride=stride)\n","\tdf['recId'] = df['sceneId'].copy()\n","\tdf['sceneId'] = df['recId'].map(rec2scene)\n","\n","\t# To scale inD x and y values into pixel coordinates, one has to divide scene 1 with 0.00814 * 12 and the others with 0.0127 * 12\n","\t# Values from the XX_recordingMeta.csv \"orthoPxToMeter\" * 12\n","\n","\t#  I dont want pixel coordinates, I want meters\n","\t#df.x /= np.where(df.sceneId == 'scene1', 0.0127 * 12, 0.00814 * 12)\n","\t#df.y /= np.where(df.sceneId == 'scene1', 0.0127 * 12, 0.00814 * 12)\n","\n","\treturn df\n","\n","def to_standard_schema(df):   # Convert to basic dataframe structure\n","\n","    df.rename(columns={\"recId\": \"case_id\", \"metaId\": \"object_id\", \"heading\":\"psi_rad\", \"frame\":\"frame_id\", \"xVelocity\":\"vx\",\"yVelocity\":\"vy\"}, inplace=True)\n","    df.drop(columns=['sceneId','trackId'], inplace=True)\n","    # Reorder columns\n","    df = df[['case_id', 'object_id', 'frame_id','x','y', 'vx', 'vy', 'psi_rad']]\n","\n","    return df\n"]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","\n","    train_recordings = ['00','01'] #'01', '02', '03', '04','07','08','09','10','11','12','13','14','15','18','19','20','21','22','23','24','25','26','27','30']\n","    val_recordings = ['05', '16','28','31']\n","    test_recordings = ['06', '17','29','32']\n","\n","    df_train = load_and_window_inD(step=12, window_size=20, stride=20, scenes=None, recordings= train_recordings, path='/content/drive/Othercomputers/My Laptop/github-repositories/transformer-multi/data/inD-dataset-v1.0/data')\n","    df_train = to_standard_schema(df_train)\n","    df_train.to_pickle('/content/drive/Othercomputers/My Laptop/github-repositories/transformer-multi/data/inD-dataset-v1.0/data/inD_train.pkl')"],"metadata":{"id":"cgTKXjNyJaPR","executionInfo":{"status":"ok","timestamp":1696325739857,"user_tz":-120,"elapsed":6199,"user":{"displayName":"victor sergio peÃ±aloza","userId":"17418028661329428856"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["    df_val = load_and_window_inD(step=12, window_size=20, stride=20, scenes=None, recordings=val_recordings, path='/content/drive/Othercomputers/My Laptop/github-repositories/ind-transformer/data/inD-dataset-v1.0/data')\n","    df_val = to_standard_schema(df_val)\n","    df_val.to_pickle('/content/drive/Othercomputers/My Laptop/github-repositories/ind-transformer/data/inD-dataset-v1.0/data/inD_val.pickle')"],"metadata":{"id":"BH6je5qnW9yU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["    df_test = load_and_window_inD(step=12, window_size=20, stride=20, scenes=None, recordings=test_recordings, path='/content/drive/Othercomputers/My Laptop/github-repositories/ind-transformer/data/inD-dataset-v1.0/data')\n","    df_test = to_standard_schema(df_test)\n","    df_test.to_pickle('/content/drive/Othercomputers/My Laptop/github-repositories/ind-transformer/data/inD-dataset-v1.0/data/inD_test.pickle')"],"metadata":{"id":"Xuzg8ETzXyBk"},"execution_count":null,"outputs":[]}]}